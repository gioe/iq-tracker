# IQ Tracker - Project Plan

## 1. Project Overview & Goals

### Vision
An iOS application that scientifically tracks users' IQ scores over time through periodic testing with fresh, AI-generated questions.

### MVP Scope
**This plan represents the Minimum Viable Product (MVP).** Future iterations will expand features and functionality based on user feedback and product evolution.

### Core Features (MVP)
- Periodic IQ testing (every few months via push notifications)
- Gamified, engaging test-taking experience
- Historical score tracking and trend visualization
- Novel questions (users never see the same question twice)
- Insights into cognitive performance over time

### Success Criteria (MVP)
- Users can complete valid IQ tests on iOS
- Test results are stored and displayed with trends
- Question pool continuously grows with high-quality AI-generated questions
- Users receive timely notifications for retesting
- No question repetition per user
- App is publishable to Apple App Store

### Key Constraints
- Must be publishable to Apple App Store
- Questions must be scientifically valid for IQ assessment
- System must scale to handle growing question pool and user base

### Future Considerations
Features and enhancements beyond MVP scope will be documented and prioritized after initial release.

---

## 2. Architecture & System Design

### High-Level Architecture

```
┌─────────────────┐
│   iOS App       │
│   (SwiftUI)     │
└────────┬────────┘
         │ HTTPS/REST
         │
┌────────▼────────────────────────────────┐
│         Backend API                     │
│  - User Management                      │
│  - Question Serving                     │
│  - Response Storage                     │
│  - Results Calculation                  │
│  - Push Notification Scheduling         │
└────────┬────────────────────────────────┘
         │
         │ Database Queries
         │
┌────────▼────────────┐         ┌──────────────────────┐
│     Database        │◄────────│  Question Service    │
│  - Users            │         │  - Multi-LLM Gen     │
│  - Questions        │         │  - Quality Arbiter   │
│  - User-Questions   │         │  - Periodic Runner   │
│  - Responses        │         └──────────────────────┘
│  - Test Results     │
└─────────────────────┘
```

### Component Interactions

1. **iOS ↔ Backend**: REST API over HTTPS
   - Authentication (JWT or similar)
   - Question fetching (filtered by user's history)
   - Response submission
   - Results retrieval

2. **Backend ↔ Database**: Direct database connection
   - User CRUD operations
   - Question retrieval with filtering
   - Response and result storage

3. **Question Service ↔ Database**: Direct database connection
   - Independent write access for new questions
   - Read access to check for duplicates

4. **Backend → iOS**: Push Notifications
   - APNs (Apple Push Notification service)
   - Scheduled test reminders

### Data Flow: Taking a Test

1. User opens app → Backend checks last test date
2. If due, Backend fetches N questions user hasn't seen
3. iOS presents questions in gamified UI
4. User submits answers → Backend stores responses
5. Backend calculates score → Stores result
6. iOS displays result + historical trends

### Question Service Execution
- **Mode**: Scheduled execution (not continuous)
- **Schedule**: TBD - to be discussed during implementation

### Deployment Strategy
- **Status**: TBD - requires discussion before decisions are made
- **iOS App**: Distributed via Apple App Store
- **Backend**: Cloud hosting (specific provider TBD)
- **Database**: Managed database service (specific choice TBD)
- **Question Service**: Scheduled job via cloud scheduler or cron

---

## 3. Component Breakdown

### 3.1 iOS App

**Technology Stack:**
- SwiftUI (modern Swift UI framework)
- MVVM architecture (Model-View-ViewModel)
- iOS 15+ target (TBD - specific minimum version to be decided)
- Swift Package Manager for dependencies

**Key Responsibilities:**
- User authentication and session management
- Gamified test-taking interface
- Question display with interactive UI
- Answer collection and local storage
- Batch answer submission
- Results visualization (current score + historical trends)
- Push notification handling
- Local data caching (optional, for offline viewing of past results)

**Core Screens/Views:**
- Welcome/Login screen
- Home/Dashboard (shows next test date, historical trends)
- Test-taking flow (question presentation)
- Results screen (immediate feedback after test)
- History/Analytics view (trends over time)
- Settings/Profile

**Key Features:**
- Engaging, gamified UX for answering questions
- Smooth animations and transitions
- Clear data visualization for trends
- Push notification opt-in and management

---

### 3.2 Backend API

**Technology Stack:**
- **Language/Framework**: TBD - requires discussion (options: Node.js/Express, Python/FastAPI, Go/Gin, etc.)
- **Database**: TBD - requires discussion (PostgreSQL, MongoDB, etc.)
- **Authentication**: JWT or similar token-based auth

**Key Responsibilities:**
- User registration and authentication
- Session management
- Question serving with user-specific filtering (never repeat questions)
- Response validation and storage
- IQ score calculation
- Test result aggregation and storage
- Historical data retrieval for trends
- Push notification scheduling via APNs
- API rate limiting and security

**API Endpoints (Planned):**

**Auth:**
- `POST /auth/register` - Create new user account
- `POST /auth/login` - Authenticate user
- `POST /auth/refresh` - Refresh auth token
- `POST /auth/logout` - Invalidate session

**User:**
- `GET /user/profile` - Get user profile
- `PUT /user/profile` - Update profile
- `GET /user/test-status` - Check if test is due

**Questions/Testing:**
- `GET /test/start` - Begin new test (returns N unseen questions)
- `POST /test/submit` - Submit all test responses in batch
- `GET /test/results/:testId` - Get specific test result
- `GET /test/history` - Get all historical results

**Notifications:**
- `POST /notifications/register-device` - Register device for push notifications
- `PUT /notifications/preferences` - Update notification settings

**Test Submission Approach:**
- **MVP Strategy**: Batch submission
- User receives all questions at test start
- Answers collected locally in iOS app
- All answers submitted together upon completion
- Trade-off: Simpler implementation and better UX over real-time submission
- Future consideration: Add integrity measures (time limits, answer locking) in later iterations

---

### 3.3 Question Generation Service

**Technology Stack:**
- **Language**: TBD - requires discussion (Python likely candidate for LLM integrations)
- **LLM Providers**: Multiple providers for diversity (OpenAI, Anthropic, etc.)
- **Execution**: Scheduled job (cron/cloud scheduler)

**Key Responsibilities:**
- Generate batches of candidate IQ questions using multiple LLMs
- Evaluate question quality using arbiter LLM
- Check for duplicate questions (similarity matching)
- Insert approved questions into database with metadata
- Log generation metrics and approval rates

**Question Generation Pipeline:**
1. **Generation Phase**: Multiple LLMs generate candidate questions
2. **Evaluation Phase**: Arbiter LLM scores each question on:
   - Clarity and lack of ambiguity
   - Appropriate difficulty
   - Validity as IQ test question
   - Proper formatting
3. **Deduplication Phase**: Check against existing questions
4. **Storage Phase**: Insert approved questions with metadata

**Question Metadata:**
- Question type (pattern recognition, logic, spatial, math, verbal, etc.)
- Difficulty level (easy, medium, hard)
- Correct answer
- Generation timestamp
- Approval score from arbiter
- Source LLM

**Monitoring & Metrics:**
- Questions generated per run
- Approval/rejection rates by LLM
- Question pool size and distribution across types/difficulties
- Execution duration and errors

---

## 4. Data Models & Database Schema

### Core Entities

#### Users
```
users
- id (primary key)
- email (unique, indexed)
- password_hash
- first_name
- last_name
- created_at
- last_login_at
- notification_enabled (boolean - opt-in/opt-out only)
- apns_device_token (for push notifications)
```

**Note:** Notification frequency is system-wide, not per-user. All users receive notifications on the same cadence (decided by system, not user).

#### Questions
```
questions
- id (primary key)
- question_text
- question_type (enum: pattern, logic, spatial, math, verbal, memory)
- difficulty_level (enum: easy, medium, hard)
- correct_answer
- answer_options (JSON - for multiple choice, or null for open-ended)
- explanation (optional - why this answer is correct)
- metadata (JSON - flexible field for additional data)
- source_llm (which LLM generated this)
- arbiter_score (quality score from arbiter LLM)
- created_at
- is_active (boolean - can be deactivated if found to be problematic)
```

#### User_Questions (Junction Table)
```
user_questions
- id (primary key)
- user_id (foreign key → users.id)
- question_id (foreign key → questions.id)
- seen_at (timestamp)
- unique constraint on (user_id, question_id)
- indexed on user_id for fast lookups
```

**Purpose:** Tracks which questions each user has seen to prevent repetition.

#### Test_Sessions
```
test_sessions
- id (primary key)
- user_id (foreign key → users.id)
- started_at
- completed_at (nullable - null if abandoned)
- status (enum: in_progress, completed, abandoned)
```

#### Responses
```
responses
- id (primary key)
- test_session_id (foreign key → test_sessions.id)
- user_id (foreign key → users.id)
- question_id (foreign key → questions.id)
- user_answer
- is_correct (boolean)
- answered_at
```

#### Test_Results
```
test_results
- id (primary key)
- test_session_id (foreign key → test_sessions.id)
- user_id (foreign key → users.id)
- iq_score (calculated IQ score)
- total_questions
- correct_answers
- completion_time_seconds
- completed_at
```

### Relationships

```
users (1) ──── (many) test_sessions
users (1) ──── (many) responses
users (1) ──── (many) test_results
users (many) ──── (many) questions [through user_questions]

test_sessions (1) ──── (many) responses
test_sessions (1) ──── (1) test_results

questions (1) ──── (many) responses
questions (many) ──── (many) users [through user_questions]
```

### Key Queries

**Get unseen questions for user:**
```sql
SELECT * FROM questions
WHERE id NOT IN (
  SELECT question_id FROM user_questions WHERE user_id = ?
)
AND is_active = true
LIMIT N
```

**Get user's test history:**
```sql
SELECT * FROM test_results
WHERE user_id = ?
ORDER BY completed_at DESC
```

**Check if user is due for test:**
```sql
SELECT completed_at FROM test_results
WHERE user_id = ?
ORDER BY completed_at DESC
LIMIT 1
```

### Indexes (Performance)
- `users.email` - unique index for login lookups
- `user_questions(user_id, question_id)` - composite unique index
- `user_questions.user_id` - for filtering unseen questions
- `test_results.user_id` - for user history queries
- `questions.is_active` - for active question filtering
- `questions.question_type` - for filtering by type (if needed)

### Data Integrity Considerations
- Foreign key constraints to maintain referential integrity
- Unique constraints on user_questions to prevent duplicates
- Transaction support for test submission (responses + results must be atomic)
- Soft deletes (is_active flag) rather than hard deletes for questions

### System Configuration

**Testing Cadence:**
- **Frequency**: 6 months between tests
- **Scope**: System-wide (applies to all users)
- **Rationale**: Aligns with typical health checkup patterns, reduces practice effects, provides meaningful data points for cognitive tracking
- **Implementation**: Configured in backend application settings (not database)
- **Future**: Can be adjusted system-wide based on data and user feedback

---

## 5. Key Technical Decisions

### 5.1 Backend Language & Framework

**Decision: Python + FastAPI**

**Rationale:**
- Excellent for ML/AI integrations (useful for IQ scoring algorithms, LLM integrations)
- FastAPI is modern, fast, and has great async support
- Strong typing with Pydantic
- Great for rapid development
- Large ecosystem of data science libraries
- Same ecosystem as Question Generation Service for consistency

**Alternatives considered:**
- Node.js + Express: Good async performance but less ideal for ML/AI tasks
- Go + Gin: Excellent performance but smaller ecosystem for LLM integrations

---

### 5.2 Database

**Decision: PostgreSQL**

**Rationale:**
- Our data is inherently relational (users, questions, responses, results)
- Strong ACID guarantees (critical for test results integrity)
- Excellent support for complex queries (filtering unseen questions, aggregations)
- JSON support for flexible fields (answer_options, metadata)
- Battle-tested, reliable, industry standard
- Great indexing and performance tuning options

**Alternatives considered:**
- MongoDB: Flexible schema but weaker consistency and less ideal for relational data

---

### 5.3 Question Generation & Evaluation Architecture

**Multi-LLM Generation with Specialized Arbiters**

**Generator LLMs (multiple for diversity):**
- OpenAI (GPT-4, GPT-4-turbo)
- Anthropic (Claude 3.5 Sonnet, Claude 3 Opus)
- Google (Gemini Pro)
- Any LLM can generate any question type

**Arbiter Architecture:**
- **Approach**: Specialized arbiters based on question type
- **Rationale**: Different models excel at different reasoning tasks per public benchmarks
- **Implementation**: Configurable mapping of question types to arbiter models

**Question Type → Arbiter Model Mapping:**
```
Question Type          → Arbiter Model
--------------------------------------------
pattern_recognition   → TBD (configurable)
logical_reasoning     → TBD (configurable)
spatial_reasoning     → TBD (configurable)
mathematical          → TBD (configurable)
verbal_reasoning      → TBD (configurable)
memory                → TBD (configurable)
```

**Configuration Approach:**
- Mapping stored in configuration file (YAML/JSON)
- Can be updated without code changes
- Each question type can have different arbiter
- Arbiter selection based on public benchmark performance

**Example Configuration Structure:**
```yaml
arbiters:
  mathematical:
    model: "gpt-4"
    provider: "openai"
    rationale: "Strong performance on MATH benchmark"
  logical_reasoning:
    model: "claude-3-5-sonnet"
    provider: "anthropic"
    rationale: "Excellent reasoning capabilities"
  # ... other mappings
```

**Pending Decisions:**
- [ ] Analyze public benchmarks (MMLU, GSM8K, ARC, Big-Bench, etc.)
- [ ] Map our question types to benchmark categories
- [ ] Assign specific arbiter models based on benchmark performance
- [ ] Determine if mapping should be static or periodically updated

---

### 5.4 Authentication & Security

**Authentication:**
- JWT (JSON Web Tokens) for stateless auth
- Bcrypt for password hashing
- Refresh token mechanism for long-lived sessions

**Security:**
- HTTPS only for all API communication
- API rate limiting
- Input validation and sanitization
- Prepared statements for SQL injection prevention

---

### 5.5 iOS Technical Decisions

**Minimum iOS Version:**
- iOS 16+ (released Sept 2022)
- Rationale: Modern SwiftUI features, good user coverage

**Push Notifications:**
- APNs (Apple Push Notification service) - required for iOS
- Backend schedules notifications, sends to APNs

---

### 5.6 Question Generation Service

**Language:** Python
- Same as backend for consistency
- Excellent LLM SDK support (OpenAI, Anthropic, Google SDKs)
- Easy to integrate with PostgreSQL

**Execution Mode:** Scheduled (not continuous)
- Cron job or cloud scheduler
- Specific schedule: TBD during implementation

---

### Summary of Decisions

| Component | Decision | Status |
|-----------|----------|--------|
| Backend Framework | Python + FastAPI | ✅ Decided |
| Database | PostgreSQL | ✅ Decided |
| Generator LLMs | Multiple (OpenAI, Anthropic, Google) | ✅ Decided |
| Arbiter Approach | Specialized per question type | ✅ Decided |
| Arbiter Mappings | Configurable | ✅ Architecture decided, specific mappings TBD |
| Authentication | JWT + Bcrypt | ✅ Decided |
| iOS Minimum Version | iOS 16+ | ✅ Decided |
| Question Service Language | Python | ✅ Decided |
| Testing Cadence | 6 months | ✅ Decided |

---

## 6. Implementation Phases & Roadmap

### Phase 1: Foundation & Infrastructure Setup

**Goal:** Set up all foundational infrastructure and project scaffolding

**Tasks:**
- [x] **P1-001**: Create monorepo structure
- [x] **P1-002**: Initialize git repository
- [x] **P1-003**: Create project documentation (README, PLAN.md)
- [x] **P1-004**: Set up PostgreSQL database (local dev environment)
- [x] **P1-005**: Create database schema and migrations
- [x] **P1-006**: Set up Python virtual environment for backend
- [x] **P1-007**: Initialize FastAPI project structure
- [x] **P1-008**: Set up Python virtual environment for question-service
- [x] **P1-009**: Create Xcode project for iOS app
- [x] **P1-010**: Configure development environment documentation
- [x] **P1-011**: Set up pre-commit hooks for backend (black, flake8, mypy)
- [x] **P1-012**: Set up pre-commit hooks for iOS (SwiftLint, SwiftFormat)
- [x] **P1-013**: Set up pre-commit hooks for question-service (black, flake8, mypy)
- [x] **P1-014**: Set up GitHub Actions CI/CD (run tests on PRs)

**Dependencies:** None (starting point)

---

### Phase 2: Backend API - Core Functionality

**Goal:** Build essential backend API with authentication and question serving

**Tasks:**
- [x] **P2-001**: Implement database models (SQLAlchemy ORM)
- [x] **P2-002**: Set up Alembic for database migrations
- [x] **P2-003**: Implement user authentication (JWT + Bcrypt)
- [x] **P2-004**: Build auth endpoints (register, login, refresh, logout)
- [x] **P2-005**: Implement user profile endpoints (GET, PUT)
- [x] **P2-006**: Build question serving logic (filter unseen questions)
- [x] **P2-007**: Implement test session management
- [x] **P2-008**: Build response submission endpoint
- [x] **P2-009**: Implement IQ score calculation algorithm
- [x] **P2-010**: Build test results storage and retrieval endpoints
- [x] **P2-011**: Add API documentation (Swagger/OpenAPI via FastAPI)
- [x] **P2-012**: Write unit tests for core backend logic
- [x] **P2-013**: Add API rate limiting
- [x] **P2-014**: Implement input validation and security measures

**Dependencies:** Phase 1 complete

---

### Phase 3: iOS App - Core UI & Authentication

**Goal:** Build iOS app foundation with authentication and navigation

**Tasks:**
- [x] **P3-001**: Set up MVVM architecture structure
- [x] **P3-002**: Create networking layer (API client)
- [x] **P3-003**: Implement authentication service
- [x] **P3-004**: Build Welcome/Login screen
- [x] **P3-005**: Build Registration screen
- [x] **P3-006**: Implement secure token storage (Keychain)
- [x] **P3-007**: Create main navigation structure
- [x] **P3-008**: Build Dashboard/Home view (placeholder)
- [x] **P3-009**: Implement logout functionality
- [x] **P3-010**: Add error handling and user feedback
- [x] **P3-011**: Write unit tests for ViewModels

**Dependencies:** Phase 2 (auth endpoints) complete

---

### Phase 4: iOS App - Test Taking Experience

**Goal:** Build engaging, gamified test-taking interface

**Tasks:**
- [x] **P4-001**: Design question display UI/UX
- [x] **P4-002**: Implement question fetching from API
- [x] **P4-003**: Build answer input components (multiple formats)
- [x] **P4-004**: Create progress indicators
- [x] **P4-005**: Add animations and transitions
- [x] **P4-006**: Implement local answer storage during test
- [x] **P4-007**: Build test submission logic
- [x] **P4-008**: Create results screen (immediate feedback)
- [x] **P4-009**: Add error handling for network issues
- [x] **P4-010**: Implement test abandonment handling
- [x] **P4-011**: User testing and UX refinement (comprehensive review completed - see Phase 10)


**Dependencies:** Phase 2 (question/test endpoints) complete

---

### Phase 5: iOS App - History & Analytics

**Goal:** Display historical test results and trends over time

**Tasks:**
- [x] **P5-001**: Build test history view
- [x] **P5-002**: Implement trend visualization (charts/graphs)
- [x] **P5-003**: Create detailed result view for past tests
- [x] **P5-004**: Add date filtering and sorting
- [x] **P5-005**: Implement insights/analytics display
- [x] **P5-006**: Design empty states (for new users)
- [x] **P5-007**: Add data refresh mechanisms
- [x] **P5-008**: Optimize performance for large datasets

**Dependencies:** Phase 4 complete

---

### Phase 6: Question Generation Service - MVP

**Goal:** Build automated question generation pipeline

**Tasks:**
- [x] **P6-001**: Set up LLM provider integrations (OpenAI SDK)
- [x] **P6-002**: Set up LLM provider integrations (Anthropic SDK)
- [x] **P6-003**: Set up LLM provider integrations (Google SDK)
- [x] **P6-004**: Create configurable arbiter mapping system (YAML/JSON config)
- [x] **P6-005**: Implement question generation pipeline (generator phase)
- [x] **P6-006**: Build arbiter evaluation logic
- [x] **P6-007**: Implement deduplication checking
- [x] **P6-008**: Create database insertion logic for approved questions
- [x] **P6-009**: Add logging and monitoring
- [x] **P6-010**: Research public benchmarks and create initial arbiter configuration
- [x] **P6-011**: Test question generation with manual review
- [x] **P6-012**: Create scheduling mechanism (cron/cloud scheduler)
- [x] **P6-013**: Document configuration and operation

**Dependencies:** Phase 1 (database) complete

---

### Phase 7: Push Notifications

**Goal:** Implement periodic test reminder notifications

**Tasks:**
- [x] **P7-001**: Set up APNs configuration and certificates
- [x] **P7-002**: Implement device token registration endpoint in backend
- [x] **P7-003**: Build notification scheduling logic (6-month cadence)
- [x] **P7-004**: Implement APNs integration in backend
- [x] **P7-005**: Add notification handling in iOS app
- [x] **P7-006**: Build notification preferences UI
- [x] **P7-007**: Test notification delivery end-to-end
- [x] **P7-008**: Handle notification permissions and edge cases

**Dependencies:** Phase 3 (iOS auth) complete

---

### Phase 8: Integration, Testing & Polish

**Goal:** End-to-end testing, bug fixes, and polish

**Tasks:**
- [x] **P8-001**: Conduct end-to-end testing (full user journey)
- [x] **P8-002**: Fix bugs identified during testing
- [x] **P8-003**: Performance optimization (API response times)
- [x] **P8-004**: Performance optimization (iOS animations and rendering)
- [x] **P8-005**: Security audit and fixes
- [x] **P8-006**: Add analytics/logging for monitoring
- [x] **P8-007**: Write integration tests
- [x] **P8-008**: Accessibility improvements (iOS VoiceOver, Dynamic Type)
- [x] **P8-009**: UI/UX polish and refinements
- [x] **P8-010**: Documentation updates
- [ ] **P8-011**: User acceptance testing

**Dependencies:** All previous phases complete

---

### Phase 9: Deployment & Launch

**Goal:** Deploy to production and submit to App Store

**Tasks:**
- [x] **P9-001**: Choose cloud hosting provider (AWS/GCP/Azure)
- [ ] **P9-002**: Set up production database
- [ ] **P9-003**: Set up CI/CD pipelines
- [ ] **P9-004**: Deploy backend to production
- [ ] **P9-005**: Deploy question generation service
- [ ] **P9-006**: Configure environment variables and secrets management
- [ ] **P9-007**: Set up monitoring and alerting (error tracking, uptime)
- [ ] **P9-008**: Prepare App Store listing (screenshots, description, keywords)
- [ ] **P9-009**: App Store submission and review
- [ ] **P9-010**: Production smoke testing
- [ ] **P9-011**: Launch!

**Dependencies:** Phase 8 complete

---

### Phase 10: UX Improvements & Polish

**Goal:** Enhance user experience based on comprehensive app review and user testing

**Priority 1: Critical Fixes (Blocking MVP)**
- [ ] **P10-001**: Add "Start Test" functionality to Dashboard
- [ ] **P10-002**: Implement History data fetching from API
- [ ] **P10-003**: Create HistoryViewModel for state management
- [ ] **P10-004**: Design and implement History list item component
- [ ] **P10-005**: Add loading states to Dashboard and History views
- [ ] **P10-006**: Enhance empty states to be more actionable and encouraging
- [ ] **P10-007**: Implement pull-to-refresh for History view

**Priority 2: Core UX Enhancements**
- [ ] **P10-008**: Create detailed test breakdown view (question-by-question review)
- [ ] **P10-009**: Add share functionality for test results (share sheet)
- [ ] **P10-010**: Implement haptic feedback for important actions
- [ ] **P10-011**: Add VoiceOver optimization and accessibility labels throughout app
- [ ] **P10-012**: Implement Dynamic Type support verification
- [ ] **P10-013**: Add accessibility identifiers for UI testing
- [ ] **P10-014**: Verify color contrast for accessibility compliance
- [ ] **P10-015**: Add time tracking display during test-taking
- [ ] **P10-016**: Implement score comparison with previous tests
- [ ] **P10-017**: Add percentile ranking display

**Priority 3: Visual Polish & Animations**
- [ ] **P10-018**: Add entrance animations to Dashboard and History views
- [ ] **P10-019**: Implement skeleton screens for loading states
- [ ] **P10-020**: Add celebration animation/confetti for high scores (>130 IQ)
- [ ] **P10-021**: Create shimmer effect for loading skeletons
- [ ] **P10-022**: Add smooth chart animations for score history
- [ ] **P10-023**: Enhance empty state visuals with illustrations
- [ ] **P10-024**: Implement custom tab bar design

**Priority 4: Additional Features**
- [ ] **P10-025**: Build onboarding flow for first-time users
- [ ] **P10-026**: Add Dark/Light mode toggle in Settings
- [ ] **P10-027**: Implement test preferences (question count, difficulty)
- [ ] **P10-028**: Add privacy settings and data export
- [ ] **P10-029**: Create account deletion functionality
- [ ] **P10-030**: Add notification preferences in Settings
- [ ] **P10-031**: Implement sound and haptic preferences
- [ ] **P10-032**: Add offline mode for viewing past results
- [ ] **P10-033**: Create sync indicator when returning online
- [ ] **P10-034**: Add "Review Answers" screen before test submission

**Priority 5: Gamification & Engagement**
- [ ] **P10-035**: Design and implement achievement system
- [ ] **P10-036**: Add streak tracking for consistent testing
- [ ] **P10-037**: Create score milestone badges
- [ ] **P10-038**: Implement daily goals system
- [ ] **P10-039**: Add motivational messages based on performance

**Priority 6: Form & Validation Improvements**
- [ ] **P10-040**: Add validation checkmarks for valid form fields
- [ ] **P10-041**: Implement password strength meter
- [ ] **P10-042**: Add email typo suggestions (e.g., gmial.com → gmail.com)
- [ ] **P10-043**: Implement progressive disclosure of form requirements

**Priority 7: Testing & Quality**
- [ ] **P10-044**: Add UI tests for critical user paths
- [ ] **P10-045**: Implement snapshot tests for visual regression
- [ ] **P10-046**: Integrate analytics for user behavior tracking
- [ ] **P10-047**: Add crash reporting integration
- [ ] **P10-048**: Conduct performance profiling and optimization

**Dependencies:** Phase 4 complete; Phase 5 recommended

---

### Phase 11: IQ Methodology - Quick Wins (Immediate Improvements)

**Goal:** Address low-hanging fruit to improve scientific validity before/at launch

**Context:** Based on comprehensive research into IQ testing methodology (see IQ_TEST_RESEARCH_FINDINGS.txt and IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt), we identified 12 divergences from scientific standards. This phase addresses the easiest, highest-impact improvements.

**Research References:**
- IQ_TEST_RESEARCH_FINDINGS.txt: Standard deviation IQ method, test composition, percentile rankings
- IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt: Divergences #1 (scoring), #8 (test composition), #9 (percentiles), #11 (score cap)

**Tasks:**

**Scoring Improvements:**
- [ ] **P11-001**: Remove artificial IQ score cap (50-150) from scoring.py
  - Current: Scores clamped to [50, 150]
  - Target: Allow full normal distribution range
  - Impact: Removes unprofessional limitation
  - Effort: 5 minutes
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, Divergence #11

- [ ] **P11-002**: Implement percentile calculation from IQ scores
  - Add percentile conversion function using z-score
  - Formula: percentile = norm.cdf((IQ - 100) / 15) * 100
  - Add percentile_rank field to test_results table
  - Effort: 3 hours
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, Divergence #9

- [ ] **P11-003**: Update API to return percentile with IQ score
  - Modify test results endpoint to include percentile
  - Add interpretation text (e.g., "Higher than 84% of population")
  - Effort: 1 hour
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 3.3

**Test Composition Improvements:**
- [ ] **P11-004**: Define standard test composition configuration
  - Document: 20 questions total
  - Distribution: 30% easy (6), 40% medium (8), 30% hard (6)
  - Domain balance: ~3-4 questions per cognitive domain
  - Create TEST_COMPOSITION constant in config
  - Effort: 1 hour
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, Divergence #8

- [ ] **P11-005**: Implement stratified question selection algorithm
  - Replace random selection with balanced selection
  - Ensure difficulty distribution (easy/medium/hard)
  - Ensure domain distribution (pattern/logic/spatial/math/verbal/memory)
  - Update question serving endpoint
  - Effort: 4-6 hours
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 5.4

- [ ] **P11-006**: Add test composition metadata to test sessions
  - Store actual composition in test_sessions.metadata JSON field
  - Track for analysis of test difficulty variance
  - Effort: 2 hours
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 890-905

**Database Schema for Future Validation:**
- [ ] **P11-007**: Add question statistics fields to database
  - Add columns: empirical_difficulty (float), discrimination (float), response_count (int)
  - Add columns: irt_difficulty, irt_discrimination, irt_guessing (for future IRT)
  - Migration script
  - Effort: 2 hours
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, Divergence #3

- [ ] **P11-008**: Add confidence interval fields to test_results
  - Add columns: standard_error (float), ci_lower (int), ci_upper (int)
  - Prepare for Phase 12 when we can calculate actual SEM
  - Effort: 1 hour
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, Divergence #7

**Analytics Infrastructure:**
- [ ] **P11-009**: Implement question performance tracking
  - Track correct/incorrect responses per question
  - Calculate p-value (proportion correct) as data accumulates
  - Calculate item-total correlation for discrimination
  - Create analytics queries/views
  - Effort: 1 week
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.6 (IRT/CTT)

- [ ] **P11-010**: Create question quality dashboard (backend admin)
  - View question statistics (p-value, discrimination, response count)
  - Flag questions where empirical difficulty != LLM-assigned difficulty
  - Identify underperforming questions
  - Effort: 1 week
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 395-420

**Messaging & Positioning:**
- [ ] **P11-011**: Update app disclaimers and positioning
  - Change "IQ Test" → "Cognitive Performance Assessment"
  - Add disclaimer: "For personal insight and tracking only, not clinical use"
  - Update App Store description, onboarding, results screen
  - Clear messaging about limitations vs professional IQ tests
  - Effort: 2-3 hours
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 518-531

**iOS Updates:**
- [ ] **P11-012**: Display percentile rankings in iOS app
  - Update results screen to show percentile
  - Add visual representation (e.g., "Top 16%")
  - Update history view to show percentiles
  - Effort: 3-4 hours
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, Divergence #9

**Dependencies:** Phase 2 (Backend API) complete

**Timeline:** 1-2 weeks

---

### Phase 12: IQ Methodology - Data Collection & Validation (MVP+)

**Goal:** Collect empirical data and establish basic psychometric validation

**Context:** After launch with Phase 11 improvements, begin collecting user response data to enable proper statistical validation. This phase focuses on gathering evidence for reliability and validity.

**Research References:**
- IQ_TEST_RESEARCH_FINDINGS.txt: Part 2.3 (Pilot Testing), Part 2.5 (Validation)
- IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt: Divergences #3 (calibration), #4 (validation), #7 (confidence intervals)

**Timeline:** 3-6 months post-launch

**Tasks:**

**Reliability Analysis:**
- [ ] **P12-001**: Calculate Cronbach's Alpha (internal consistency)
  - Requires: 100+ completed tests with common questions
  - Calculate alpha coefficient for test reliability
  - Target: α ≥ 0.70 (acceptable), α ≥ 0.90 (excellent)
  - Effort: 2-3 days (statistical analysis)
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 4.1

- [ ] **P12-002**: Implement test-retest reliability tracking
  - Identify users who take multiple tests
  - Calculate correlation between test scores
  - Minimum time gap: 2-4 weeks recommended
  - Target: r > 0.7 (good)
  - Effort: 2-3 days
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 4.1

- [ ] **P12-003**: Calculate Standard Error of Measurement (SEM)
  - Formula: SEM = SD × √(1 - α)
  - Use population SD and Cronbach's alpha
  - Enables confidence interval calculation
  - Effort: 1 day
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 680-710

- [ ] **P12-004**: Implement confidence intervals in scoring
  - Calculate CI using SEM: CI = IQ ± (1.96 × SEM)
  - Store ci_lower and ci_upper in test_results
  - Update API responses to include confidence intervals
  - Effort: 2-3 days
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 3.4

- [ ] **P12-005**: Update iOS app to display confidence intervals
  - Change from "IQ: 115" to "IQ: 115 (range: 109-121)"
  - Add tooltip explaining confidence intervals
  - Show ranges in history view
  - Effort: 1 week
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 715-745

**Question Calibration (Classical Test Theory):**
- [ ] **P12-006**: Empirical difficulty calculation (p-values)
  - For each question: p = correct_count / total_attempts
  - Requires: 100+ responses per question minimum
  - Compare to LLM-assigned difficulty
  - Update empirical_difficulty field
  - Effort: 1 week
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.6 (CTT)

- [ ] **P12-007**: Item discrimination analysis
  - Calculate point-biserial correlation (item vs total score)
  - Target: r > 0.3 acceptable, r > 0.4 good
  - Flag items with low discrimination for review
  - Update discrimination field
  - Effort: 1 week
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.6

- [ ] **P12-008**: Question quality review process
  - Review questions where empirical_difficulty deviates from target
  - Review questions with discrimination < 0.3
  - Revise or deactivate problematic questions
  - Document decisions
  - Effort: Ongoing, 4-6 hours monthly
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 425-455

**Validation Studies:**
- [ ] **P12-009**: Document psychometric properties
  - Compile reliability coefficients (alpha, test-retest)
  - Calculate mean scores and standard deviations
  - Document question statistics
  - Create technical report
  - Effort: 1-2 weeks
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 4

- [ ] **P12-010**: Establish interim scoring improvements
  - Use collected data to refine scoring algorithm
  - Adjust for question difficulty if sufficient data
  - Still preparing for full norming in Phase 13
  - Effort: 2-3 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 175-215

**Data Infrastructure:**
- [ ] **P12-011**: Create psychometric analysis pipeline
  - Automated calculation of reliability metrics
  - Scheduled jobs to update question statistics
  - Reporting dashboard for monitoring test quality
  - Effort: 2-3 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 460-485

**Dependencies:**
- Phase 11 complete (data collection infrastructure in place)
- 100+ users with completed tests
- Multiple months of user data

**Success Criteria:**
- Cronbach's α > 0.70
- Test-retest correlation r > 0.5
- 80%+ of questions have 100+ responses
- SEM calculated and confidence intervals implemented

---

### Phase 13: IQ Methodology - Norming & Proper IQ Scoring

**Goal:** Implement scientifically valid deviation IQ scoring with population norms

**Context:** With sufficient user data (500-1000+ users), establish population norms and implement proper standard deviation IQ scoring method. This is the critical phase for achieving scientific validity.

**Research References:**
- IQ_TEST_RESEARCH_FINDINGS.txt: Part 3 (Scoring Formulas), Part 2.4 (Norming)
- IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt: Divergences #1 (scoring formula), #2 (norming sample)

**Timeline:** 6-12 months post-launch

**Prerequisites:**
- 500-1000+ users with completed tests (minimum for stable norms)
- Diverse user base (age, education, geography)
- Multiple test cycles completed (6+ months of data)

**Tasks:**

**Norming Sample Collection:**
- [ ] **P13-001**: Collect demographic data for norming
  - Add optional demographic fields to user profiles
  - Age, education level, geographic location
  - Voluntary participation in norming study
  - Ensure privacy compliance
  - Effort: 1 week (UI + backend)
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.4

- [ ] **P13-002**: Validate norming sample representativeness
  - Analyze demographic distribution
  - Compare to general population statistics
  - Identify any sampling biases
  - Document limitations
  - Effort: 2-3 weeks (statistical analysis)
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 225-260

- [ ] **P13-003**: Calculate population statistics
  - Calculate mean (μ) of raw scores across norming sample
  - Calculate standard deviation (σ) of raw scores
  - Validate normal distribution assumption
  - Store in configuration/database
  - Effort: 1 week
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 3.1

**Deviation IQ Scoring Implementation:**
- [ ] **P13-004**: Implement proper z-score calculation
  - z = (X - μ) / σ where X = individual raw score
  - Handle edge cases (σ = 0, outliers)
  - Unit tests for z-score calculation
  - Effort: 3-4 days
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 3.1

- [ ] **P13-005**: Implement deviation IQ formula
  - IQ = 100 + (15 × z)
  - Replace existing StandardIQRangeScoring class
  - Create DeviationIQScoring class
  - Effort: 3-4 days
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 50-120

- [ ] **P13-006**: Create scoring strategy switcher
  - Configuration to toggle between old and new scoring
  - A/B testing capability
  - Gradual rollout support
  - Effort: 1 week
  - Reference: backend/app/core/scoring.py (existing strategy pattern)

- [ ] **P13-007**: Validate new scoring algorithm
  - Compare old vs new scores on historical data
  - Ensure distribution is normal (mean=100, SD=15)
  - Check for edge cases and outliers
  - Statistical validation
  - Effort: 2-3 weeks
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 3.3

**Score Migration:**
- [ ] **P13-008**: Recalculate historical scores
  - Apply new scoring to all existing test results
  - Store both old and new scores for comparison
  - Migration script with rollback capability
  - Effort: 1 week (careful implementation required)
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 910-925

- [ ] **P13-009**: Update user-facing scores
  - Transition from old to new scoring in UI
  - Explain score changes to users
  - Provide mapping/comparison
  - Handle user communications
  - Effort: 1-2 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 518-531

**Ongoing Norming:**
- [ ] **P13-010**: Implement continuous norming system
  - Update population statistics as user base grows
  - Periodic recalculation (quarterly/annually)
  - Detect and handle Flynn effect (score inflation)
  - Version norm sets
  - Effort: 3-4 weeks
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.4

- [ ] **P13-011**: Create norm monitoring dashboard
  - Track population mean and SD over time
  - Detect significant shifts requiring renorming
  - Alert system for anomalies
  - Effort: 1-2 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 260-280

**Documentation & Communication:**
- [ ] **P13-012**: Document norming methodology
  - Technical documentation of norming process
  - Sample characteristics and limitations
  - Scoring methodology explanation
  - Publish psychometric properties
  - Effort: 2-3 weeks
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 7

- [ ] **P13-013**: Update app messaging with new claims
  - Can now claim "standardized IQ scoring"
  - Update disclaimers appropriately
  - Still note limitations vs clinical tests
  - Effort: 1 week
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 1450-1470

**Dependencies:**
- Phase 12 complete (reliability established)
- 500-1000+ users with completed tests
- Diverse, representative user base

**Success Criteria:**
- Proper deviation IQ formula implemented and validated
- Population norms established with documented methodology
- Score distribution follows normal curve (mean=100, SD=15)
- Historical scores successfully migrated
- Psychometric properties documented and published

---

### Phase 14: IQ Methodology - Advanced Psychometrics (Optional)

**Goal:** Achieve research-grade psychometric quality with IRT and formal validation

**Context:** This phase represents the gold standard for IQ assessment - IRT-based scoring, formal validation studies, and potential academic publication. Optional for commercial product but necessary for research or clinical applications.

**Research References:**
- IQ_TEST_RESEARCH_FINDINGS.txt: Part 2.3 (IRT), Part 4 (Validation Standards)
- IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt: Divergence #6 (IRT), Divergence #4 (validation)

**Timeline:** 12+ months post-launch

**Prerequisites:**
- Phase 13 complete (deviation IQ scoring in place)
- 200+ responses per question (for stable IRT parameters)
- 1000+ users (for robust validation studies)
- Budget for external validation (optional)

**Tasks:**

**Item Response Theory (IRT) Implementation:**
- [ ] **P14-001**: Research and select IRT library/approach
  - Evaluate: py-irt, mirt (via rpy2), pyirt
  - Choose 2PL or 3PL model
  - Proof of concept implementation
  - Effort: 2-3 weeks
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.3.2

- [ ] **P14-002**: Prepare response matrix for IRT analysis
  - Extract user × question response data
  - Format for IRT library
  - Data quality validation
  - Effort: 1-2 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 650-675

- [ ] **P14-003**: Calibrate IRT item parameters
  - Estimate a (discrimination), b (difficulty), c (guessing) for each question
  - Validate model fit
  - Identify poorly fitting items
  - Store parameters in database
  - Effort: 4-6 weeks (includes learning curve)
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.3.3

- [ ] **P14-004**: Implement IRT-based ability estimation
  - Maximum Likelihood Estimation (MLE) or EAP/MAP
  - Replace sum scoring with IRT theta estimation
  - Convert theta to IQ scale: IQ = 100 + (15 × theta)
  - Effort: 4-6 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 620-665

- [ ] **P14-005**: Implement Test Information Function
  - Calculate precision across ability range
  - Compute person-specific SEM
  - Display measurement precision to users
  - Effort: 2-3 weeks
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.3.6

- [ ] **P14-006**: Validate IRT scoring vs CTT scoring
  - Compare IRT and deviation IQ scores
  - Correlation should be high (r > 0.9)
  - Document improvements in measurement precision
  - Effort: 2-3 weeks
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 620-650

**Formal Validation Studies:**
- [ ] **P14-007**: Design test-retest reliability study
  - Recruit 50-100 participants for formal study
  - Administer test twice (2-4 weeks apart)
  - IRB approval if publishing
  - Effort: 3-6 months (study execution)
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 4.1

- [ ] **P14-008**: Conduct reliability study and analysis
  - Calculate test-retest correlation
  - Calculate confidence intervals
  - Compare to professional IQ test standards
  - Document methodology and results
  - Effort: 2-3 months
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 490-545

- [ ] **P14-009**: Design concurrent validity study (optional, expensive)
  - Partner with research institution
  - Participants take both our test and established IQ test (WAIS/Stanford-Binet)
  - Calculate correlation
  - Target: r > 0.70 with established tests
  - Effort: 6-12 months, significant cost
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 4.2

- [ ] **P14-010**: Conduct concurrent validity study
  - Recruit participants
  - Administer both tests
  - Statistical analysis
  - Document results and limitations
  - Effort: 6-12 months
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 545-560

**Expert Consultation:**
- [ ] **P14-011**: Engage psychometrician for review
  - Expert review of methodology
  - Statistical validation of approach
  - Recommendations for improvement
  - Effort: Consulting engagement, 1-3 months
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 930-945

- [ ] **P14-012**: Peer review preparation (if pursuing publication)
  - Write technical paper documenting methodology
  - Describe psychometric properties
  - Submit to journal (e.g., Journal of Psychoeducational Assessment)
  - Effort: 6-12 months
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 7

**Advanced Features:**
- [ ] **P14-013**: Computer Adaptive Testing (CAT) research
  - Investigate adaptive question selection
  - Select next question based on current ability estimate
  - Reduce test length while maintaining precision
  - Proof of concept
  - Effort: 3-6 months
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 2.3.7

- [ ] **P14-014**: Implement CAT (if validated)
  - Real-time ability estimation during test
  - Dynamic question selection algorithm
  - Stopping rule (precision threshold or question limit)
  - Effort: 6-12 months
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 780-820

**Documentation & Publication:**
- [ ] **P14-015**: Publish comprehensive psychometric report
  - Reliability coefficients
  - Validity evidence
  - IRT parameters and model fit
  - Norming sample characteristics
  - Limitations and appropriate uses
  - Effort: 2-3 months
  - Reference: IQ_TEST_RESEARCH_FINDINGS.txt, Part 4

- [ ] **P14-016**: Seek professional endorsement (optional)
  - APA guidelines compliance
  - Professional organization review
  - Certification/accreditation if applicable
  - Effort: 6-12 months
  - Reference: IQ_METHODOLOGY_DIVERGENCE_ANALYSIS.txt, lines 950-975

**Dependencies:**
- Phase 13 complete (norming established)
- Sufficient data for IRT (200+ responses per question)
- Budget for validation studies and expert consultation

**Success Criteria:**
- IRT parameters estimated for all active questions
- Test-retest reliability r > 0.80
- Internal consistency α > 0.90
- Published psychometric properties
- Optional: Concurrent validity r > 0.70 with established tests

**Note:** This phase is optional for commercial product. Implement if:
- Pursuing research/academic use cases
- Seeking clinical applications
- Targeting enterprise/educational market requiring validation
- Building long-term scientific credibility

---

### Current Status

**Completed:**
- ✅ **Phase 1: Foundation & Infrastructure Setup** (14/14 tasks - 100%)
  - Monorepo structure and documentation (P1-001 to P1-003)
  - PostgreSQL database setup with schema and migrations (P1-004, P1-005)
  - Python virtual environments for backend and question-service (P1-006, P1-008)
  - FastAPI project structure initialized (P1-007)
  - Xcode iOS project created (P1-009)
  - Development environment documentation (P1-010)
  - Pre-commit hooks for all components (P1-011, P1-012, P1-013)
  - GitHub Actions CI/CD (P1-014)

- ✅ **Phase 2: Backend API - Core Functionality** (14/14 tasks - 100%)
  - Database models and migrations (P2-001, P2-002)
  - User authentication system with JWT (P2-003, P2-004)
  - User profile management (P2-005)
  - Question serving with filtering (P2-006)
  - Test session management (P2-007)
  - Response submission and storage (P2-008)
  - IQ score calculation (P2-009)
  - Test results retrieval (P2-010)
  - API documentation (P2-011)
  - Unit tests (P2-012)
  - Rate limiting (P2-013)
  - Input validation and security measures (P2-014)

**In Progress:**
- None - Phases 1 and 2 complete!

**Next Steps:**
- Begin Phase 3 (iOS App - Core UI & Authentication)
- Begin Phase 4 (iOS App - Test Taking Experience)
- Begin Phase 6 (Question Generation Service - MVP)

---

### Adding New Tasks

When new tasks are identified:
1. Add to appropriate phase with next sequential ID (e.g., `P2-015`)
2. Update PLAN.md immediately
3. Reference by ID in discussions and commits

---

## 7. Open Questions

### IQ Testing & Scoring

**Q1: How do we calculate IQ scores from test responses?**
- What scoring algorithm should we use?
- Should it be normalized against a population?
- Do we need different scoring for different question types?
- What's a scientifically valid approach for our use case?
- **Priority**: Must answer before Phase 2 (P2-009)

**Q2: How many questions per test?**
- Standard IQ tests vary (20-60+ questions)
- Need to balance: test validity vs user engagement/time
- Consideration: 20-30 questions for ~15-20 minute test?
- **Priority**: Must answer before Phase 2 (P2-006)

**Q3: How do we ensure our tests are scientifically valid?**
- Should we consult with psychologists or IQ testing experts?
- Do we need to validate against existing standardized tests?
- What's our threshold for "good enough" for MVP vs future rigor?
- **Priority**: Important for credibility, can refine post-MVP

**Q4: Should question difficulty be adaptive?**
- MVP: Fixed difficulty distribution
- Future: Adjust difficulty based on user performance?
- **Priority**: Defer to post-MVP

---

### Question Generation

**Q5: What's the question generation schedule frequency?**
- Daily? Weekly? Monthly?
- How many questions should we generate per run?
- Depends on: user growth rate, questions per test, test frequency
- **Priority**: Must answer during Phase 6 (P6-012)

**Q6: How do we seed the initial question pool?**
- Question generation service needs to run before users can take tests
- How many questions do we need before launch? (100? 500? 1000?)
- Should we manually create/review the first batch?
- **Priority**: Must answer before Phase 6 completion

**Q7: What are the specific arbiter model mappings?**
- Need to research benchmarks (MMLU, GSM8K, ARC, Big-Bench)
- Map question types to best-performing models
- Document in arbiter configuration file
- **Priority**: Addressed by P6-010

**Q8: How do we handle deduplication?**
- Exact match checking is easy
- Semantic similarity for near-duplicates? (use embeddings?)
- What similarity threshold constitutes a "duplicate"?
- **Priority**: Must answer during Phase 6 (P6-007)

---

### User Experience

**Q9: What happens when a user runs out of unseen questions?**
- With 6-month cadence, users would need ~20-30 new questions every 6 months
- Question pool should grow faster than consumption
- Edge case: what if pool exhausted? Allow repeats after X time? Show message?
- **Priority**: Nice to handle for MVP, can defer

**Q10: What's the user onboarding flow?**
- Can users take a test immediately after signing up?
- Should there be an intro/tutorial?
- Do we explain how IQ tracking works?
- **Priority**: Design during Phase 3

**Q11: How do we handle test abandonment?**
- User starts test but doesn't finish
- Save progress? Allow resume? Or discard?
- MVP decision: Mark as abandoned, don't allow resume (simpler)
- **Priority**: Addressed in Phase 4 (P4-010)

---

### Business & Compliance

**Q12: What's the pricing/monetization model?**
- Free for MVP?
- Future: subscription, one-time purchase, ads, freemium?
- Does this affect our architecture decisions?
- **Priority**: Defer to post-MVP

**Q13: Privacy and data handling**
- What data do we collect?
- GDPR/CCPA compliance needed?
- Privacy policy requirements
- User data export/deletion capabilities
- **Priority**: Must address before App Store submission (Phase 9)

**Q14: App Store requirements**
- What categories/keywords for App Store listing?
- Age rating considerations
- Required privacy disclosures
- Beta testing approach (TestFlight)
- **Priority**: Must answer during Phase 9 (P9-008)

---

### Technical Details

**Q15: How do we handle API versioning?**
- `/v1/` prefix from the start?
- How to handle breaking changes in future?
- MVP decision: Start with `/v1/` endpoints
- **Priority**: Implement in Phase 2

**Q16: What analytics/metrics should we track?**
- User engagement (DAU, MAU, retention)
- Test completion rates
- Question quality metrics
- API performance
- Error rates
- **Priority**: Basic logging for MVP, advanced analytics post-MVP

**Q17: Disaster recovery and backups**
- Database backup strategy
- How often? Where stored?
- Recovery time objective (RTO)?
- **Priority**: Must answer during Phase 9 deployment

**Q18: Development vs Production environments**
- Need separate databases for dev/staging/prod
- Environment configuration management
- How to test push notifications without spamming prod users?
- **Priority**: Address during Phase 1 and Phase 9

---

### Decision Summary

**Must Answer Before Launch:**
- Q1: IQ scoring algorithm (Phase 2)
- Q2: Questions per test (Phase 2)
- Q6: Initial question pool seeding (Phase 6)
- Q7: Arbiter model mappings (Phase 6)
- Q13: Privacy policy (Phase 9)
- Q17: Backup strategy (Phase 9)

**MVP Decisions Made:**
- Q11: No test resume for abandoned tests
- Q15: Use `/v1/` API versioning from start

**Can Defer to Post-MVP:**
- Q4: Adaptive difficulty
- Q12: Pricing model
- Q16: Advanced analytics

---

## 8. Development Practices & Workflow

### Testing Standards

**Philosophy:** Focus on testing critical paths rather than arbitrary coverage percentages.

**Backend (Python/FastAPI):**
- Use `pytest` for all tests
- Test critical paths:
  - Authentication flow (registration, login, token refresh)
  - Question serving logic (filtering unseen questions)
  - Test submission and scoring
  - Data integrity (responses, results storage)
- Integration tests for API endpoints
- Mock external dependencies (LLM APIs, etc.)
- Test database setup with fixtures

**iOS (Swift/SwiftUI):**
- Use `XCTest` for unit tests
- Test critical paths:
  - ViewModels business logic
  - API client networking layer
  - Authentication service
  - Local data persistence
  - Answer submission logic
- UI tests optional for MVP (focus on unit tests)

**Question Service (Python):**
- Use `pytest` for all tests
- Test critical paths:
  - Question generation pipeline
  - Arbiter evaluation logic
  - Deduplication checking
  - Database insertion
- Mock LLM API responses for deterministic testing

---

### Git Workflow & Branch Strategy

**Branch Strategy:**
- `main` branch = stable, deployable code (protected)
- Feature branches = `feature/P#-###-brief-description`
  - Example: `feature/P2-003-jwt-auth`
  - One branch per task ID
  - Create from latest `main`

**Workflow:**
1. **ALWAYS start by pulling latest main:** `git checkout main && git pull origin main`
2. Start work on a task (e.g., P2-003)
3. Create feature branch from updated main: `git checkout -b feature/P2-003-jwt-auth`
4. Make multiple commits as needed (atomic, logical commits)
5. **Final commit: Update PLAN.md to check off task:** `- [x] P2-003`
6. Push branch to GitHub
7. Create Pull Request
8. PR gets reviewed on GitHub (async)
9. Address feedback if needed (additional commits are fine)
10. Merge to `main` after approval
11. Delete feature branch

**IMPORTANT:** Always ensure you've pulled the latest changes from main before creating a new feature branch. This prevents merge conflicts from working on outdated code.

**Why update PLAN.md before the PR?** This ensures the work and status update merge together atomically. The checkbox in `main` always accurately reflects what's actually been completed and merged.

**Multiple commits per task:** Encouraged! Break work into logical, atomic commits within a feature branch.

**One PR per task:** Each task ID gets its own PR.

---

### Pull Request (PR) Guidelines

**PR Title Format:**
```
[P#-###] Brief task description
```

Example: `[P2-003] Implement JWT authentication`

**PR Description Template:**
```markdown
## Task
Closes P2-003: Implement user authentication (JWT + Bcrypt)

## Changes
- Added JWT token generation and validation
- Implemented bcrypt password hashing
- Created auth endpoints (login, register, refresh, logout)
- Added unit tests for auth service

## Testing
- [ ] Unit tests pass locally
- [ ] Manual testing completed
- [ ] No breaking changes

## Notes
(Any additional context, decisions made, or follow-up items)
```

**Review Process:**
- Author creates PR (no pre-review needed)
- Reviewer (you) reviews on GitHub asynchronously
- Reviewer approves or requests changes
- Author addresses feedback
- Merge when approved

---

### Commit Message Format

**Format:**
```
[P#-###] Brief description of change

Optional longer explanation if needed.
Can include multiple lines of context.
```

**Examples:**
```
[P2-003] Add JWT token generation utility

Implements JWT encoding/decoding with expiration handling.
Uses PyJWT library with HS256 algorithm.
```

```
[P2-003] Create auth endpoints for login and registration
```

```
[P1-005] Add database migration for users table
```

---

### Code Quality & Linting

**Pre-commit Hooks:** Automatically run before each commit

**Backend & Question Service (Python):**
- `black` - Code formatting (opinionated, consistent style)
- `flake8` - Linting (PEP 8 compliance)
- `mypy` - Static type checking
- Configuration files: `pyproject.toml`, `.flake8`

**iOS (Swift):**
- `SwiftLint` - Linting (style and conventions)
- `SwiftFormat` - Code formatting
- Configuration file: `.swiftlint.yml`

**Tasks:**
- P1-011: Set up pre-commit hooks for backend
- P1-012: Set up pre-commit hooks for iOS
- P1-013: Set up pre-commit hooks for question-service

---

### Continuous Integration (CI)

**GitHub Actions:** Automated testing on pull requests

**CI Pipeline (runs on every PR):**
1. Checkout code
2. Set up environment (Python/Swift)
3. Install dependencies
4. Run linters
5. Run tests
6. Report results

**Required Checks:**
- All tests must pass
- Linting must pass
- No merge to `main` without passing CI

**Task:**
- P1-014: Set up GitHub Actions CI/CD workflow

---

### Code Review Checklist

Before approving a PR, verify:

- [ ] Tests pass (CI green)
- [ ] Code follows project conventions
- [ ] No obvious security vulnerabilities
- [ ] Critical paths are tested
- [ ] Code is readable and maintainable
- [ ] No sensitive data (API keys, passwords) committed
- [ ] Documentation updated if needed (README, API docs)
- [ ] PLAN.md checkbox is checked off in the PR

---

### Development Environment

**Local Setup:**
- Each developer maintains own local PostgreSQL instance
- `.env` files for local configuration (gitignored)
- Sample `.env.example` files committed to repo
- Virtual environments for Python projects
- Xcode for iOS development

**Environment Variables:**
- Never commit secrets or API keys
- Use `.env` files locally
- Use environment variable management for production (P9-006)

**Documentation:**
- Development setup instructions in component READMEs
- P1-010: Comprehensive dev environment documentation
