# Arbiter Model Selection Guide

## Table of Contents
- [Overview](#overview)
- [Current Configuration](#current-configuration)
- [Selection Methodology](#selection-methodology)
- [Benchmark Data Sources](#benchmark-data-sources)
- [Update Process](#update-process)
- [Decision Criteria](#decision-criteria)
- [Change History](#change-history)

---

## Overview

This document explains the research-driven process for selecting specialized arbiter models for each question type in the IQ Tracker question generation pipeline. Each arbiter model is chosen based on public benchmark performance in areas relevant to the question type it evaluates.

### Arbiter Purpose

Arbiters evaluate candidate questions generated by various LLMs, scoring them on:
- **Clarity** (25%): Question is clear and unambiguous
- **Difficulty** (20%): Appropriate difficulty for IQ testing
- **Validity** (30%): Valid as an IQ test question
- **Formatting** (15%): Properly formatted with correct answer
- **Creativity** (10%): Novel and interesting question

Questions scoring above 0.7 are approved for inclusion in the question pool.

---

## Current Configuration

**Last Updated**: January 2025
**Configuration Version**: 1.0
**Based on**: 2024 benchmark data

### Question Type Assignments

| Question Type | Arbiter Model | Provider | Key Benchmarks |
|--------------|---------------|----------|----------------|
| **mathematical** | gpt-4-turbo | OpenAI | GSM8K: 95.0%, MATH: 60.1% |
| **logical_reasoning** | claude-3-5-sonnet-20241022 | Anthropic | HumanEval: 93.7%, GPQA: 67.2% |
| **pattern_recognition** | claude-3-5-sonnet-20241022 | Anthropic | GPQA: 67.2%, MMLU: 90.4% |
| **spatial_reasoning** | claude-3-5-sonnet-20241022 | Anthropic | GPQA: 67.2%, MMLU: 90.4% |
| **verbal_reasoning** | claude-3-5-sonnet-20241022 | Anthropic | MMLU: 90.4%, HellaSwag: ~90% |
| **memory** | claude-3-5-sonnet-20241022 | Anthropic | MMLU: 90.4%, 200K context |
| **default** | gpt-4-turbo | OpenAI | General-purpose fallback |

### Key Findings

**Claude 3.5 Sonnet dominates** most question types due to:
- Exceptional reasoning performance (GPQA: 67.2%, far exceeding GPT-4's 35.7%)
- Strong knowledge retention (MMLU: 90.4%)
- Superior coding/logical reasoning (HumanEval: 93.7%)
- Massive context window (200K tokens) beneficial for memory tasks

**GPT-4 Turbo retained** for:
- Mathematical questions (strong GSM8K: 95.0% performance)
- Default fallback (broad competence across tasks)

---

## Selection Methodology

### Question Type → Benchmark Mapping

Each question type was mapped to relevant public benchmarks:

#### 1. Mathematical
**Relevant Benchmarks**:
- **GSM8K** (Grade School Math 8K): Elementary to middle-school level math word problems
- **MATH**: Competition-level mathematics from AMC, AIME, and other contests

**Decision Logic**: Mathematical questions require strong quantitative reasoning. GSM8K tests basic problem-solving, while MATH evaluates advanced mathematical thinking.

**Winner**: GPT-4 Turbo (95.0% GSM8K, 60.1% MATH)

#### 2. Logical Reasoning
**Relevant Benchmarks**:
- **HumanEval**: Coding problems requiring logical decomposition
- **GPQA** (Graduate-Level Google-Proof Q&A): Complex reasoning tasks
- **ARC** (AI2 Reasoning Challenge): Science reasoning questions

**Decision Logic**: Logical reasoning requires systematic thinking, inference, and deductive capabilities tested by coding and graduate-level problems.

**Winner**: Claude 3.5 Sonnet (HumanEval: 93.7%, GPQA: 67.2%)

#### 3. Pattern Recognition
**Relevant Benchmarks**:
- **GPQA**: Abstract reasoning patterns
- **MMLU**: Pattern identification across domains
- **ARC**: Pattern-based science reasoning

**Decision Logic**: Pattern recognition requires identifying abstract relationships and structures. GPQA's high difficulty makes it ideal for testing sophisticated pattern detection.

**Winner**: Claude 3.5 Sonnet (GPQA: 67.2%, MMLU: 90.4%)

#### 4. Spatial Reasoning
**Relevant Benchmarks**:
- **GPQA**: Contains spatial reasoning components
- **MMLU**: Includes geometry and spatial tasks
- **Spatial-specific benchmarks**: Limited public data available

**Decision Logic**: While spatial-specific benchmarks are limited in public data, general reasoning strength (GPQA) correlates with spatial reasoning ability.

**Winner**: Claude 3.5 Sonnet (GPQA: 67.2%, strong general reasoning)

#### 5. Verbal Reasoning
**Relevant Benchmarks**:
- **MMLU**: Comprehensive language understanding across 57 subjects
- **HellaSwag**: Commonsense language reasoning
- **WinoGrande**: Coreference resolution and language understanding

**Decision Logic**: Verbal reasoning requires nuanced language understanding, vocabulary, and comprehension across diverse contexts.

**Winner**: Claude 3.5 Sonnet (MMLU: 90.4%, HellaSwag: ~90%)

#### 6. Memory
**Relevant Benchmarks**:
- **MMLU**: Knowledge recall and retention
- **Context window size**: Proxy for information retention capability

**Decision Logic**: Memory questions test recall and retention. MMLU measures factual knowledge, while large context windows indicate better information management.

**Winner**: Claude 3.5 Sonnet (MMLU: 90.4%, 200K token context window)

---

## Benchmark Data Sources

### Primary Benchmarks

#### MMLU (Massive Multitask Language Understanding)
- **Description**: 57 subjects spanning STEM, humanities, social sciences
- **Format**: Multiple-choice questions
- **Link**: https://github.com/hendrycks/test

#### GSM8K (Grade School Math 8K)
- **Description**: 8,500 grade school math word problems
- **Format**: Free-form numerical answers
- **Link**: https://github.com/openai/grade-school-math

#### MATH
- **Description**: 12,500 competition mathematics problems
- **Difficulty**: High school to undergraduate level
- **Link**: https://github.com/hendrycks/math

#### HumanEval
- **Description**: 164 programming problems
- **Format**: Function implementation tasks
- **Link**: https://github.com/openai/human-eval

#### GPQA (Graduate-Level Google-Proof Q&A)
- **Description**: Graduate-level reasoning questions resistant to search
- **Difficulty**: PhD-level across multiple domains
- **Link**: Research benchmark (published 2023)

#### HellaSwag
- **Description**: Commonsense natural language inference
- **Format**: Sentence completion tasks
- **Link**: https://rowanzellers.com/hellaswag/

#### WinoGrande
- **Description**: Commonsense reasoning (Winograd Schema Challenge)
- **Format**: Pronoun resolution tasks
- **Link**: https://winogrande.allenai.org/

### Leaderboards & Resources

- **Vellum LLM Leaderboard**: https://www.vellum.ai/llm-leaderboard
- **Papers with Code**: https://paperswithcode.com/
- **Hugging Face Open LLM Leaderboard**: https://huggingface.co/spaces/open-llm-leaderboard
- **Official model announcements**: OpenAI, Anthropic, Google blogs
- **Academic papers**: Model technical reports and research publications

### 2024 Benchmark Scores (Reference)

**Claude 3.5 Sonnet (claude-3-5-sonnet-20241022)**:
- MMLU: 90.4%
- HumanEval: 93.7%
- GPQA: 67.2%
- GSM8K: ~95%

**GPT-4 Turbo**:
- MMLU: ~88.7%
- GSM8K: 95.0%
- MATH: 60.1%
- HumanEval: ~90%

**GPT-4o**:
- MMLU: 88.7%
- MATH: 76.6%
- HumanEval: 90.2%

**Gemini 1.5 Pro**:
- MMLU: ~88.6%
- HellaSwag: 93.3%
- HumanEval: 71.9%

---

## Update Process

### Update Schedule

**Recommended Frequency**: **Quarterly** (every 3 months)

**Rationale**:
- Major model releases: 2-4 times per year
- Benchmark updates: Continuous, but meaningful changes occur quarterly
- Question quality: Won't degrade rapidly with slightly outdated arbiters
- Operational balance: Staying current without excessive overhead

### When to Update

Perform a configuration review when:
1. **Scheduled quarterly review** (March, June, September, December)
2. **Major model release** (GPT-5, Claude 4, Gemini 2.0, etc.)
3. **New benchmark publication** that significantly changes rankings
4. **Production quality issues** traced to arbiter performance

### Update Workflow

#### Step 1: Gather Latest Benchmark Data

```bash
# Visit benchmark leaderboards
1. Vellum LLM Leaderboard: https://www.vellum.ai/llm-leaderboard
2. Hugging Face Open LLM Leaderboard
3. Official model documentation (OpenAI, Anthropic, Google)
4. Papers with Code: https://paperswithcode.com/
```

Create a spreadsheet or table with current scores:

| Model | MMLU | GSM8K | MATH | HumanEval | GPQA | HellaSwag |
|-------|------|-------|------|-----------|------|-----------|
| GPT-4 Turbo | | | | | | |
| Claude 3.5 Sonnet | | | | | | |
| Gemini Pro | | | | | | |
| ... | | | | | | |

#### Step 2: Analyze Performance Changes

For each question type:
1. Identify the current arbiter and its benchmark scores
2. Check if any alternative model now significantly outperforms it
3. Consider **switching threshold**: >5% improvement on primary benchmark

Example:
```
Current: claude-3-5-sonnet (GPQA: 67.2%)
Alternative: hypothetical-model-x (GPQA: 73.0%)
Improvement: +5.8% → CONSIDER SWITCHING
```

#### Step 3: Update Configuration File

If switching is warranted:

```bash
cd question-service
nano config/arbiters.yaml  # or use your preferred editor
```

Update the relevant section:
```yaml
logical_reasoning:
  model: "new-model-name"
  provider: "provider-name"
  rationale: "Updated reasoning: [benchmark scores]. Previous: claude-3-5-sonnet (GPQA: 67.2%). New model shows significant improvement..."
  benchmark_data:
    HumanEval: "XX.X%"
    GPQA: "XX.X%"
  enabled: true
```

#### Step 4: Document the Change

Add entry to this document's [Change History](#change-history) section:

```markdown
### [Date] - Configuration Update

**Changed**: [question_type] arbiter: [old_model] → [new_model]

**Reason**: New model shows [X]% improvement on [benchmark_name]

**Benchmark Comparison**:
- Old: [benchmark scores]
- New: [benchmark scores]

**Impact**: Expected improvement in [aspect of question evaluation]
```

#### Step 5: Test Configuration

```bash
cd question-service
source venv/bin/activate
python -m pytest tests/  # Ensure tests pass
python examples/arbiter_config_example.py  # Verify config loads correctly
```

#### Step 6: Deploy Update

```bash
git add config/arbiters.yaml docs/ARBITER_SELECTION.md
git commit -m "[ARBITER] Update arbiter configuration - [date]"
git push origin main
```

If question service is already deployed:
```bash
# Restart the service to pick up new configuration
# (Specific commands depend on deployment method)
```

#### Step 7: Monitor Question Quality

After deployment:
- Review generated questions for 1-2 weeks
- Check arbiter approval rates
- Compare question quality subjectively
- Revert if quality degrades

---

## Decision Criteria

### When to Switch Arbiter Models

Use the following criteria to decide if an arbiter change is warranted:

#### Primary Criteria: Benchmark Performance
- **Threshold**: >5% improvement on primary benchmark(s)
- **Multiple benchmarks**: Improvement across multiple relevant benchmarks
- **Consistency**: Performance maintained across diverse tasks in the domain

#### Secondary Considerations

1. **Cost**: New model's API pricing vs. current model
   - Only switch if cost increase is <20% OR performance gain >10%

2. **Latency**: Response time for arbiter evaluations
   - Avoid models >2x slower unless performance gain is substantial (>15%)

3. **API Reliability**: Uptime and rate limits of provider
   - Ensure provider meets our SLA requirements

4. **Model Availability**: Stable API access
   - Prefer generally available models over limited beta access

5. **Context Window**: Sufficient for question + evaluation prompt
   - Minimum 4K tokens required; prefer 8K+

#### Special Cases

**Gemini Models**: Currently not selected as primary arbiters despite competitive benchmarks due to:
- Smaller context window options in some variants
- Less consistent performance across diverse tasks
- May reconsider if Gemini 2.0+ shows sustained superiority

**GPT-4o vs GPT-4 Turbo**:
- GPT-4o shows better MATH scores (76.6% vs 60.1%)
- Consider switching mathematical arbiter to GPT-4o if API costs are acceptable
- Currently retaining GPT-4 Turbo due to strong GSM8K performance and established reliability

#### Risk Assessment

Before switching, consider:
- **Reversibility**: Can we easily revert if quality degrades?
- **A/B Testing**: Can we test new arbiter on subset before full rollout?
- **Backup Configuration**: Maintain previous config for quick rollback

---

## Decision Criteria Summary

### Switching Checklist

Use this checklist when evaluating arbiter changes:

- [ ] New model shows >5% improvement on primary benchmark
- [ ] Performance validated across multiple relevant benchmarks
- [ ] Cost increase is acceptable (<20%) OR performance gain is substantial (>10%)
- [ ] Latency is acceptable (<2x current model)
- [ ] Provider API is reliable and meets SLA
- [ ] Model is generally available (not limited beta)
- [ ] Context window sufficient (8K+ tokens preferred)
- [ ] Configuration updated in `config/arbiters.yaml`
- [ ] Change documented in this file's Change History
- [ ] Tests pass with new configuration
- [ ] Monitoring plan in place for post-deployment quality checks

---

## Change History

### 2025-01-11 - Initial Configuration (v1.0)

**Created**: Evidence-based arbiter configuration based on 2024 benchmark research

**Model Assignments**:
- **mathematical**: gpt-4-turbo (OpenAI)
- **logical_reasoning**: claude-3-5-sonnet-20241022 (Anthropic)
- **pattern_recognition**: claude-3-5-sonnet-20241022 (Anthropic) - *Changed from gpt-4-turbo*
- **spatial_reasoning**: claude-3-5-sonnet-20241022 (Anthropic)
- **verbal_reasoning**: claude-3-5-sonnet-20241022 (Anthropic)
- **memory**: claude-3-5-sonnet-20241022 (Anthropic) - *Changed from gpt-4-turbo*

**Key Changes from Placeholder Config**:
1. Switched `pattern_recognition` from GPT-4 Turbo to Claude 3.5 Sonnet
   - Reason: Superior GPQA performance (67.2% vs 35.7% for GPT-4)
   - Expected improvement in abstract reasoning evaluation

2. Switched `memory` from GPT-4 Turbo to Claude 3.5 Sonnet
   - Reason: Higher MMLU score (90.4% vs ~88%) + massive 200K context window
   - Expected improvement in evaluating recall-based questions

**Research Sources**:
- Vellum LLM Leaderboard (2024)
- Official model documentation (OpenAI, Anthropic, Google)
- Academic benchmark publications (GPQA, MMLU, GSM8K, MATH, HumanEval)

**Next Review**: April 2025 (Q2 2025 quarterly review)

---

## Appendix: Benchmark Descriptions

### Detailed Benchmark Information

#### MMLU (Massive Multitask Language Understanding)
- **Size**: 15,908 questions across 57 subjects
- **Subjects**: Abstract algebra, US history, professional medicine, elementary mathematics, etc.
- **Format**: 4-choice multiple choice
- **Evaluation**: Zero-shot and few-shot accuracy
- **Why it matters**: Tests broad knowledge and reasoning across all cognitive domains

#### GSM8K (Grade School Math 8K)
- **Size**: 8,500 problems
- **Difficulty**: Elementary and middle school level
- **Format**: Math word problems with numerical answers
- **Why it matters**: Tests basic quantitative reasoning and problem decomposition

#### MATH
- **Size**: 12,500 problems
- **Difficulty**: Competition mathematics (AMC 10/12, AIME, etc.)
- **Format**: Free-form answers requiring symbolic math
- **Why it matters**: Tests advanced mathematical reasoning beyond basic arithmetic

#### HumanEval
- **Size**: 164 programming problems
- **Format**: Python function implementation
- **Evaluation**: Functional correctness via unit tests
- **Why it matters**: Proxy for logical reasoning, decomposition, and systematic thinking

#### GPQA (Graduate-Level Google-Proof Q&A)
- **Size**: Multiple sets (Diamond: 198 questions)
- **Difficulty**: PhD-level questions designed to be non-searchable
- **Domains**: Biology, physics, chemistry, and more
- **Why it matters**: Tests highest-level reasoning and domain expertise

#### HellaSwag
- **Size**: 70,000 scenarios
- **Format**: Sentence completion with 4 choices
- **Focus**: Commonsense natural language inference
- **Why it matters**: Tests intuitive understanding of everyday situations

#### WinoGrande
- **Size**: 44,000 problems
- **Format**: Fill-in-the-blank pronoun resolution
- **Focus**: Commonsense reasoning
- **Why it matters**: Tests subtle language understanding and context reasoning

---

## Contact & Questions

For questions about arbiter configuration or to report issues:
- **GitHub Issues**: [Link to repository issues]
- **Configuration Issues**: Check `config/arbiters.yaml` format and model availability
- **Benchmark Questions**: Refer to benchmark documentation linked in this guide

**Maintainers**: Question generation service team
**Last Updated**: January 11, 2025
