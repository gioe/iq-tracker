"""Data models for question generation.

This module defines the data structures used throughout the question
generation pipeline, from generation to evaluation.
"""

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, field_validator


class QuestionType(str, Enum):
    """Types of IQ test questions."""

    PATTERN_RECOGNITION = "pattern_recognition"
    LOGICAL_REASONING = "logical_reasoning"
    SPATIAL_REASONING = "spatial_reasoning"
    MATHEMATICAL = "mathematical"
    VERBAL_REASONING = "verbal_reasoning"
    MEMORY = "memory"


class DifficultyLevel(str, Enum):
    """Difficulty levels for questions."""

    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"


class GeneratedQuestion(BaseModel):
    """A question generated by an LLM provider.

    Attributes:
        question_text: The question text/prompt
        question_type: Type of question (pattern, logic, etc.)
        difficulty_level: Difficulty rating
        correct_answer: The correct answer
        answer_options: List of answer options for multiple choice (optional)
        explanation: Explanation of why the answer is correct (optional)
        metadata: Additional metadata about the question
        source_llm: Which LLM generated this question
        source_model: Specific model identifier
    """

    question_text: str = Field(..., min_length=10)
    question_type: QuestionType
    difficulty_level: DifficultyLevel
    correct_answer: str = Field(..., min_length=1)
    answer_options: Optional[List[str]] = None
    explanation: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    source_llm: str = Field(..., min_length=1)  # "openai", "anthropic", "google"
    source_model: str = Field(..., min_length=1)  # Specific model name

    @field_validator("answer_options")
    @classmethod
    def validate_answer_options(
        cls, v: Optional[List[str]], info
    ) -> Optional[List[str]]:
        """Validate that correct_answer is in answer_options if provided."""
        if v is not None:
            if len(v) < 2:
                raise ValueError("Must have at least 2 answer options if provided")
            correct = info.data.get("correct_answer")
            if correct and correct not in v:
                raise ValueError(
                    f"correct_answer '{correct}' must be in answer_options"
                )
        return v

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for database insertion."""
        return {
            "question_text": self.question_text,
            "question_type": self.question_type.value,
            "difficulty_level": self.difficulty_level.value,
            "correct_answer": self.correct_answer,
            "answer_options": self.answer_options,
            "explanation": self.explanation,
            "metadata": self.metadata,
            "source_llm": self.source_llm,
            "source_model": self.source_model,
        }


class EvaluationScore(BaseModel):
    """Evaluation score from an arbiter model.

    Attributes:
        clarity_score: Score for clarity and lack of ambiguity (0.0 to 1.0)
        difficulty_score: Score for appropriate difficulty (0.0 to 1.0)
        validity_score: Score for validity as IQ test question (0.0 to 1.0)
        formatting_score: Score for proper formatting (0.0 to 1.0)
        creativity_score: Score for novelty and interest (0.0 to 1.0)
        overall_score: Weighted overall score (0.0 to 1.0)
        feedback: Optional textual feedback from arbiter
    """

    clarity_score: float = Field(..., ge=0.0, le=1.0)
    difficulty_score: float = Field(..., ge=0.0, le=1.0)
    validity_score: float = Field(..., ge=0.0, le=1.0)
    formatting_score: float = Field(..., ge=0.0, le=1.0)
    creativity_score: float = Field(..., ge=0.0, le=1.0)
    overall_score: float = Field(..., ge=0.0, le=1.0)
    feedback: Optional[str] = None


class EvaluatedQuestion(BaseModel):
    """A generated question with its evaluation score.

    Attributes:
        question: The generated question
        evaluation: Evaluation score from arbiter
        arbiter_model: Which arbiter model evaluated this
        approved: Whether the question meets the threshold
    """

    question: GeneratedQuestion
    evaluation: EvaluationScore
    arbiter_model: str
    approved: bool

    @property
    def is_approved(self) -> bool:
        """Check if question is approved."""
        return self.approved


class GenerationBatch(BaseModel):
    """A batch of generated questions.

    Attributes:
        questions: List of generated questions
        question_type: Type of questions in this batch
        batch_size: Number of questions requested
        generation_timestamp: When this batch was generated
        metadata: Additional batch metadata
    """

    questions: List[GeneratedQuestion]
    question_type: QuestionType
    batch_size: int
    generation_timestamp: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

    def __len__(self) -> int:
        """Get number of questions in batch."""
        return len(self.questions)
