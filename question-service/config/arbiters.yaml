# Arbiter Model Configuration
# Maps question types to specific arbiter models for quality evaluation
#
# Each question type can be evaluated by a different model based on
# benchmark performance and domain expertise.
#
# Configuration Structure:
#   question_type:
#     model: Model identifier (e.g., "gpt-4", "claude-3-5-sonnet-20241022")
#     provider: Provider name ("openai", "anthropic", "google")
#     rationale: Explanation of why this model was chosen
#     enabled: Whether this arbiter is active (default: true)

version: "1.0"

arbiters:
  mathematical:
    model: "gpt-4-turbo"
    provider: "openai"
    rationale: "Strong performance on math benchmarks: GSM8K (95.0%), MATH (60.1%). GPT-4 Turbo excels at grade-school math and maintains competitive performance on advanced mathematical reasoning."
    benchmark_data:
      GSM8K: "95.0%"
      MATH: "60.1%"
    enabled: true

  logical_reasoning:
    model: "claude-3-5-sonnet-20241022"
    provider: "anthropic"
    rationale: "Best-in-class logical reasoning: HumanEval (93.7%), GPQA (67.2%). Claude 3.5 Sonnet demonstrates superior performance on coding logic and graduate-level reasoning tasks."
    benchmark_data:
      HumanEval: "93.7%"
      GPQA: "67.2%"
      MMLU: "90.4%"
    enabled: true

  pattern_recognition:
    model: "claude-3-5-sonnet-20241022"
    provider: "anthropic"
    rationale: "Superior abstract reasoning: GPQA (67.2%), MMLU (90.4%). Claude 3.5 Sonnet excels at identifying patterns in complex reasoning tasks, significantly outperforming alternatives."
    benchmark_data:
      GPQA: "67.2%"
      MMLU: "90.4%"
    enabled: true

  spatial_reasoning:
    model: "claude-3-5-sonnet-20241022"
    provider: "anthropic"
    rationale: "Strong general reasoning capabilities with GPQA (67.2%) and MMLU (90.4%) scores. While spatial-specific benchmarks are limited, Claude 3.5 Sonnet's reasoning strength makes it ideal for spatial tasks."
    benchmark_data:
      GPQA: "67.2%"
      MMLU: "90.4%"
    enabled: true

  verbal_reasoning:
    model: "claude-3-5-sonnet-20241022"
    provider: "anthropic"
    rationale: "Exceptional language understanding: MMLU (90.4%), strong performance on HellaSwag and WinoGrande. Claude 3.5 Sonnet leads in natural language reasoning and comprehension tasks."
    benchmark_data:
      MMLU: "90.4%"
      HellaSwag: "~90%"
    enabled: true

  memory:
    model: "claude-3-5-sonnet-20241022"
    provider: "anthropic"
    rationale: "Outstanding recall and knowledge retention: MMLU (90.4%) with 200K token context window. Claude 3.5 Sonnet combines strong knowledge benchmarks with massive context for memory-intensive evaluation."
    benchmark_data:
      MMLU: "90.4%"
      context_window: "200,000 tokens"
    enabled: true

# Default arbiter (fallback for unknown question types)
default_arbiter:
  model: "gpt-4-turbo"
  provider: "openai"
  rationale: "General-purpose fallback arbiter"
  enabled: true

# Evaluation criteria weights (0.0 to 1.0)
# These determine how heavily each factor is weighted in the arbiter score
evaluation_criteria:
  clarity: 0.25              # Question is clear and unambiguous
  difficulty: 0.20           # Appropriate difficulty for IQ testing
  validity: 0.30             # Valid as an IQ test question
  formatting: 0.15           # Properly formatted with correct answer
  creativity: 0.10           # Novel and interesting question

# Minimum score threshold for question approval (0.0 to 1.0)
min_arbiter_score: 0.7

# Notes:
# - Model identifiers should match the exact model names used by provider SDKs
# - Provider names must match: "openai", "anthropic", or "google"
# - Enabled flag can be used to temporarily disable specific arbiters
# - Evaluation criteria weights should sum to 1.0
